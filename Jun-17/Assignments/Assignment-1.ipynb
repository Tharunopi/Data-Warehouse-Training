{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64dd0c2d-1b30-4ede-a045-2f2681285cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Assignment-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dae7e24-31f9-49ed-a5a9-ef2c154ca16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64595ea0-b0bd-4e8a-a9c7-4bc5d21bec11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"dbshell-01\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b6a3cd-1d12-486a-997a-a8e69b13fe5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOrd = spark.read.csv(\"/FileStore/tables/orders.csv\", header=True, inferSchema=True)\n",
    "dfCus = spark.read.csv(\"/FileStore/tables/customers.csv\", header=True, inferSchema=True)\n",
    "dfPro = spark.read.csv(\"/FileStore/tables/products.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce08d5d-2ba7-412e-a781-5471c905293a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Pyspark + Delta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250284d2-e0b1-4acd-9ee5-00ea7bde0140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ingest all 3 CSVs as Delta Tables.\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales_info\")\n",
    "spark.sql(\"USE sales_info\")\n",
    "\n",
    "dfOrd.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_info.orders\")\n",
    "dfCus.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_info.customers\")\n",
    "dfPro.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_info.products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a2ff5d-5ae0-463f-a631-ea03574ef507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|ProductName|totalRevenue|\n+-----------+------------+\n|      Phone|      150000|\n|     Laptop|       75000|\n|     Tablet|       30000|\n|   Keyboard|       30000|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Write SQL to get the total revenue per Product.\n",
    "spark.sql(\"\"\"\n",
    "          SELECT p.ProductName, SUM(o.Quantity * o.Price) AS totalRevenue FROM products p\n",
    "          INNER JOIN orders o\n",
    "          ON o.ProductID = p.ProductID\n",
    "          GROUP BY p.ProductName\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654f5632-d7fc-41d5-9e46-e3d2e0604836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n|Region|regionRevenue|\n+------+-------------+\n| South|       100000|\n|  East|        30000|\n|  West|        30000|\n| North|       125000|\n+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Join Orders + Customers to find revenue by Region.\n",
    "dlOrd2 = DeltaTable.forName(spark, \"orders\")\n",
    "dlCus2 = DeltaTable.forName(spark, \"customers\")\n",
    "dlPro2 = DeltaTable.forName(spark, \"products\")\n",
    "dfOrd2 = dlOrd2.toDF()\n",
    "dfCus2 = dlCus2.toDF()\n",
    "dfPro2 = dlPro2.toDF()\n",
    "\n",
    "dfOrd2.join(dfCus2, on=\"CustomerID\", how=\"inner\") \\\n",
    "    .groupBy(\"Region\") \\\n",
    "    .agg(\n",
    "        F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"regionRevenue\")\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be3fa03-145c-4980-a876-000727a7c435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+---------+\n|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n+-------+----------+---------+--------+-----+----------+---------+\n|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n|   3005|      C004|    P1004|       3|10000|2024-05-05|Cancelled|\n+-------+----------+---------+--------+-----+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Update the Status of Pending orders to 'Cancelled'.\n",
    "dfOrdUpdated = dfOrd2.withColumn(\"Status\", F.when(dfOrd2.Status == 'Pending', 'Cancelled').otherwise(dfOrd2.Status))\n",
    "dfOrdUpdated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669d13a3-36d7-4279-abc8-1550ba74ae1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+---------+\n|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n+-------+----------+---------+--------+-----+----------+---------+\n|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n|   3005|      C004|    P1004|       3|10000|2024-05-05|Cancelled|\n+-------+----------+---------+--------+-----+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Merge a new return record into Orders.\n",
    "dlOrd2.alias(\"target\") \\\n",
    "    .merge(\n",
    "    dfOrdUpdated.alias(\"source\"),\n",
    "    'target.OrderID = source.OrderID'\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "upserted = dlOrd2.toDF()\n",
    "upserted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d710faf8-0324-49f6-aaab-7c2fdaa03305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **DLT Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4993e7b-4f55-4ad2-b2b2-013faf53a472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Create raw → cleaned → aggregated tables:\n",
    "# Clean: Remove rows with NULLs\n",
    "# Aggregated: Total revenue per Category\n",
    "import dlt \n",
    "\n",
    "@dlt.table\n",
    "def extract_1():\n",
    "  return spark.read.table(\"sales_data.orders\")\n",
    "\n",
    "@dlt.table\n",
    "def extract_2():\n",
    "  return spark.read.table(\"sales_data.customers\")\n",
    "\n",
    "@dlt.table\n",
    "def extract_3():\n",
    "  return spark.read.table(\"sales_data.products\")\n",
    "\n",
    "@dlt.table \n",
    "def cleaned_1():\n",
    "  return dlt.read(\"extract_1\").dropna()\n",
    "\n",
    "@dlt.table \n",
    "def cleaned_2():\n",
    "  return dlt.read(\"extract_2\").dropna()\n",
    "\n",
    "@dlt.table \n",
    "def cleaned_3():\n",
    "  return dlt.read(\"extract_3\").dropna()\n",
    "\n",
    "@dlt.table\n",
    "def agg_final():\n",
    "  orders = dlt.read(\"cleaned_1\")\n",
    "  products = dlt.read(\"cleaned_3\")\n",
    "  return orders.join(products, on=\"ProductID\", how=\"inner\") \\\n",
    "          .groupBy(\"Category\") \\\n",
    "          .agg(\n",
    "            F.sum(F.col(\"Quantity\") * F.col(\"Price\"))\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda494a2-2ff1-4491-81ae-98c4635f9300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Time Travel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3c5487-8a0d-444b-9185-6d28cd6be814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|version|operation                        |operationParameters                                                                                                                                                                    |\n+-------+---------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|8      |RESTORE                          |{version -> 5, timestamp -> null}                                                                                                                                                      |\n|7      |RESTORE                          |{version -> 5, timestamp -> null}                                                                                                                                                      |\n|6      |MERGE                            |{predicate -> [\"(OrderID#9522 = OrderID#9219)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n|5      |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}                                                                                                          |\n|4      |UPDATE                           |{predicate -> [\"(Status#5741 = Pending)\"]}                                                                                                                                             |\n|3      |UPDATE                           |{predicate -> [\"(Status#5103 = Pending)\"]}                                                                                                                                             |\n|2      |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}                                                                                                          |\n|1      |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}                                                                                                          |\n|0      |CREATE TABLE AS SELECT           |{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}                                                                                                          |\n+-------+---------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n+-------+----------+---------+--------+-----+----------+---------+\n|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n+-------+----------+---------+--------+-----+----------+---------+\n|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n+-------+----------+---------+--------+-----+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. View data before the Status update.\n",
    "history = dlOrd2.history()\n",
    "history.select([\"version\", \"operation\", \"operationParameters\"]).show(truncate=False)\n",
    "\n",
    "dlOrd2.restoreToVersion(5)\n",
    "beforeUpdate = dlOrd2.toDF()\n",
    "beforeUpdate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d6f96e-7a79-493f-8d06-4229f80e1a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
     ]
    }
   ],
   "source": [
    "# 8. Restore to an older version of the orders table.\n",
    "dlOrd2.restoreToVersion(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3eb9b8a-a92c-4e7f-9289-75d57b1a319f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Vaccum + Retention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53d9abf-ada2-45f0-95fe-27ba2fd94b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# 9. Run VACUUM after changing default retention.\n",
    "spark.sql(\"\"\"\n",
    "          ALTER TABLE orders SET TBLPROPERTIES (\n",
    "              'delta.logRetentionDuration' = '14 days',\n",
    "              'delta.deletedFileRetentionDuration' = '14 days'\n",
    "          )\n",
    "          \"\"\")\n",
    "    \n",
    "dlOrd2 = DeltaTable.forName(spark, \"orders\")\n",
    "dlOrd2.vacuum(1290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5778ac1e-e965-4b8a-b1d1-b7d8eeed8586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Expectations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f603b2c1-81ba-43fc-a7a5-541c59e5f5bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10. Quantity > 0 , Price > 0 , OrderDate is not null\n",
    "@dlt.table\n",
    "def raw():\n",
    "  return spark.read.format(\"delta\").table(\"orders\")\n",
    "\n",
    "@dlt.expect_or_drop(\"check_qty_price_date\", \"Quantity > 0 AND Price > 0 AND OrderDate IS NOT NULL\")\n",
    "def silver():\n",
    "  return dlt.read(\"raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5465f4ae-3cbd-4141-905d-1ac41f6b6ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Bonus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dcf57d4-ae11-4fad-8fd4-3dbe0992a527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+---------+---------+\n|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|OrderType|\n+-------+----------+---------+--------+-----+----------+---------+---------+\n|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|NotReturn|\n|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|   Return|\n|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|NotReturn|\n|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|NotReturn|\n|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|NotReturn|\n+-------+----------+---------+--------+-----+----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 11. Use when-otherwise to create a new column: OrderType = \"Return\" if Status == 'Returned'\n",
    "dfOrd2.withColumn(\n",
    "    \"OrderType\",\n",
    "    F.when(F.col(\"Status\") == \"Returned\", \"Return\").otherwise(\"NotReturn\")\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ce20f4-8b02-484c-be1f-6229aa5311a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment-1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}