{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd8754a-b206-4339-b336-58f8e0e7a3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7cbdad0-8066-4c4c-a01f-b8453b290b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1520322650531891#setting/sparkui/0611-042249-grg1r6w4/driver-5594160078528939871\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a2f3c1cbf80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"NB-1\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532f2583-534a-4bab-8c99-bb793e23ddbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f28d0fd-bb69-49de-9d8c-cc6582e03879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------------------------------------------------+------+------+\n|OrderID|Customer|Items                                                         |Region|Amount|\n+-------+--------+--------------------------------------------------------------+------+------+\n|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n+-------+--------+--------------------------------------------------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1},\n",
    "{\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0),\n",
    "Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}],\n",
    "Region=\"Europe\", Amount=650.0),\n",
    "Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2},\n",
    "{\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0),\n",
    "Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}],\n",
    "Region=\"US\", Amount=450.0)\n",
    "]\n",
    "df_sales = spark.createDataFrame(data)\n",
    "df_sales.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "537090ab-9c07-4c41-919f-6cb673f4ea5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Working with JSON & Nested Fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc67cdba-a0ea-48da-b9e7-01901bfd8917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+\n|OrderID|Customer|               Items|Region|Amount|\n+-------+--------+--------------------+------+------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|\n+-------+--------+--------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Flatten the Items array using explode() to create one row per product.\n",
    "df_sales_exploded = df_sales.select('*', F.explode(\"Items\").alias(\"Products\")).drop(\"Items\")\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b14b6fb-35fd-428d-97f8-cd953c184fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n|Product|TotalQuantity|\n+-------+-------------+\n| Laptop|          1.0|\n|  Mouse|          2.0|\n| Tablet|          1.0|\n|  Phone|          2.0|\n|Charger|          1.0|\n|   Desk|          1.0|\n+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Count total quantity sold per product.\n",
    "df_sales_exploded.select(\"Products.Product\", \"Products.Qty\").groupby(\"Product\").agg(\n",
    "    F.sum(\"Qty\").alias(\"TotalQuantity\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48bf49b0-117e-4620-9842-92018b2c9d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Region|OrderCount|\n+------+----------+\n|Europe|         1|\n|    US|         1|\n|  Asia|         2|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Count number of orders per region.\n",
    "df_sales_exploded.groupby(\"Region\").agg(\n",
    "    F.countDistinct(df_sales_exploded.OrderID).alias(\"OrderCount\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ed6f0c-1be6-438f-ac24-df75c49978d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Using when and otherwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7676d3-5a18-4d12-ba06-967619881876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+--------------+\n|OrderID|Customer|               Items|Region|Amount|HighOrderValue|\n+-------+--------+--------------------+------+------+--------------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|            No|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|            No|\n+-------+--------+--------------------+------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a new column HighValueOrder :\n",
    "# \"Yes\" if Amount > 1000\n",
    "# \"No\" otherwise\n",
    "df_sales.withColumn(\"HighOrderValue\", F.when(df_sales.Amount > 1000, \"Yes\").otherwise(\"No\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0912c63e-9b91-4ceb-aeb5-6abbb31ea57f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+------------+\n|OrderID|Customer|               Items|Region|Amount|ShippingZone|\n+-------+--------+--------------------+------+------+------------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|      Zone A|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|      Zone B|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|      Zone A|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|      Zone C|\n+-------+--------+--------------------+------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Add a column ShippingZone :\n",
    "# Asia → \"Zone A\", Europe → \"Zone B\", US → \"Zone C\"\n",
    "df_sales.withColumn(\"ShippingZone\", F.when(df_sales.Region == \"Asia\", \"Zone A\").when(df_sales.Region == \"Europe\", \"Zone B\").otherwise(\"Zone C\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382b4ad7-c2f7-420d-b6e3-21d553996a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Temporary & Permanent views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2401eff8-018f-4820-86b4-1ed2c50c0eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales\")\n",
    "spark.sql(\"USE sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d26070-ba48-4628-b016-93c3ab3e764d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Register df_sales as a temporary view named sales_view .\n",
    "df_sales.createOrReplaceTempView(\"sales_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0039b7a7-c139-4ffc-aac9-073ca536e02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+\n|Region|OrderCount|AverageAmount|\n+------+----------+-------------+\n|  Asia|         2|       1045.0|\n|Europe|         1|        650.0|\n|    US|         1|        450.0|\n+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a SQL query to:\n",
    "# Count orders by Region\n",
    "# Find average amount per region\n",
    "spark.sql(\"\"\"\n",
    "          SELECT Region, COUNT(OrderID) AS OrderCount, AVG(Amount) AS AverageAmount FROM sales_view\n",
    "          GROUP BY Region\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "087b18f5-399c-4213-aa36-1bd09bd9d127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Create a permanent view using saveAsTable() .\n",
    "df_sales.write.saveAsTable(\"sales.df_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6760fae-1fc1-4050-a34f-3ba59f4a794f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**SQL Queries via Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac49ca6-b5f6-4567-894e-5f389a46051a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+\n|OrderID|Customer|               Items|Region|Amount|\n+-------+--------+--------------------+------+------+\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|\n+-------+--------+--------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Use SQL to filter all orders with more than 1 item.\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM df_sales\n",
    "          WHERE SIZE(Items) > 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7248c616-c8a1-4726-8d5a-c00099ab0afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n|Customer|Amount|\n+--------+------+\n|   Mohan| 890.0|\n|     Ali|1200.0|\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Use SQL to extract customer names where Amount > 800.\n",
    "spark.sql(\"\"\"\n",
    "          SELECT Customer, Amount FROM df_sales\n",
    "          WHERE Amount > 800\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbba26eb-a578-4105-a021-67442f66e887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Saving as Parquet and Reading Again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb88095-c81c-43a4-949e-0300355ff464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11. Save the exploded product-level DataFrame as a partitioned Parquet file by Region .\n",
    "df_sales_exploded.write.mode(\"overwrite\").parquet(\"/Workspace/Shared/sales/sales_exploded\", partitionBy=\"Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9a7688-9ef4-4890-a054-d3a4715f3068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n|Product|Product|TotalQty|\n+-------+-------+--------+\n|  Phone|      1|     2.0|\n|Charger|      1|     1.0|\n| Laptop|      1|     1.0|\n|  Mouse|      1|     2.0|\n| Tablet|      1|     1.0|\n|   Desk|      1|     1.0|\n+-------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 12. Read the parquet back and perform a group-by on Product .\n",
    "df_parquet_sales = spark.read.parquet(\"/Workspace/Shared/sales/sales_exploded\")\n",
    "df_parquet_sales.groupBy(\"Products.Product\").agg(\n",
    "    F.countDistinct(\"Products.Product\").alias(\"Product\"),\n",
    "    F.sum(\"Products.Qty\").alias(\"TotalQty\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ffc59d2-0f42-40e7-adad-1e5ea86a9c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}