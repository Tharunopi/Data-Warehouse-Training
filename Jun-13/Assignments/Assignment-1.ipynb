{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd8754a-b206-4339-b336-58f8e0e7a3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window as W\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ded424-2e39-4529-a035-f52cafee07eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1520322650531891#setting/sparkui/0611-042249-grg1r6w4/driver-689818859003830489\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x746ad8f41d00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a97e28d-2901-43cf-b4c7-02fb3c895c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Spark Tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc24dc4c-c184-44ab-bd73-00d1e4dec4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ingest the CSV files into two PySpark DataFrames\n",
    "df_cus = spark.read.csv(r\"file:/Workspace/Shared/jun-13/customers.csv\", header=True, inferSchema=True)\n",
    "df_ord = spark.read.csv(r\"file:/Workspace/Shared/jun-13/orders.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a76806-570a-48cd-8abd-2eb2585b94bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CustomerID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Age: integer (nullable = true)\n\nNone\nroot\n |-- OrderID: integer (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Product: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n |-- OrderDate: date (nullable = true)\n\nNone\n"
     ]
    }
   ],
   "source": [
    "# 2. Infer schema and print the schema for both\n",
    "print(df_cus.printSchema())\n",
    "print(df_ord.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b87d4f3-9879-4688-988f-ead97cc3f848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------+-----+----------+-----------+\n|OrderID|CustomerID|Product|Quantity|Price| OrderDate|TotalAmount|\n+-------+----------+-------+--------+-----+----------+-----------+\n|   1001|       101| Laptop|       1|70000|2024-01-05|      70000|\n|   1002|       102| Mobile|       2|25000|2024-02-10|      50000|\n|   1003|       103|   Desk|       1|10000|2024-03-15|      10000|\n|   1004|       101|  Mouse|       3| 1000|2024-04-01|       3000|\n|   1005|       104|Monitor|       1|12000|2024-04-25|      12000|\n+-------+----------+-------+--------+-----+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Add a column TotalAmount = Quantity * Price to orders\n",
    "df_ord = df_ord.withColumn(\"TotalAmount\", df_ord.Quantity * df_ord.Price)\n",
    "df_ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b0d05b-3fa2-42a1-8496-62174e01ffc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+---+-------+-------+--------+-----+----------+-----------+\n|CustomerID| Name|     City|Age|OrderID|Product|Quantity|Price| OrderDate|TotalAmount|\n+----------+-----+---------+---+-------+-------+--------+-----+----------+-----------+\n|       101|Aditi|   Mumbai| 28|   1001| Laptop|       1|70000|2024-01-05|      70000|\n|       102|Rohan|    Delhi| 35|   1002| Mobile|       2|25000|2024-02-10|      50000|\n|       103|Meena|Bangalore| 41|   1003|   Desk|       1|10000|2024-03-15|      10000|\n|       101|Aditi|   Mumbai| 28|   1004|  Mouse|       3| 1000|2024-04-01|       3000|\n|       104|Kabir|Hyderabad| 30|   1005|Monitor|       1|12000|2024-04-25|      12000|\n+----------+-----+---------+---+-------+-------+--------+-----+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Join both DataFrames on CustomerID\n",
    "df_joined = df_cus.join(df_ord, on=\"CustomerID\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08921853-f67c-403f-a6fc-77afe976b3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------+-----+----------+-----------+\n|OrderID|CustomerID|Product|Quantity|Price| OrderDate|TotalAmount|\n+-------+----------+-------+--------+-----+----------+-----------+\n|   1001|       101| Laptop|       1|70000|2024-01-05|      70000|\n|   1002|       102| Mobile|       2|25000|2024-02-10|      50000|\n+-------+----------+-------+--------+-----+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Filter orders where TotalAmount > 20000\n",
    "df_ord.filter(df_ord.TotalAmount > 20_000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f884558-0a04-49f2-8b10-7d1d1c320f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|CustomerID|OrderCount|\n+----------+----------+\n|       101|         2|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Show customers who placed more than 1 order\n",
    "df_joined.groupby(\"CustomerID\").agg(\n",
    "    F.count(\"OrderID\").alias(\"OrderCount\")\n",
    ").filter(F.col(\"OrderCount\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ca9ea7-b8a0-4906-a61f-ecf8e1454222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n|     City|AverageOrderValue|\n+---------+-----------------+\n|Bangalore|          10000.0|\n|   Mumbai|          36500.0|\n|    Delhi|          50000.0|\n|Hyderabad|          12000.0|\n+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Group orders by City and get average order value\n",
    "df_joined.groupBy(\"City\").agg(\n",
    "    F.mean(\"TotalAmount\").alias(\"AverageOrderValue\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b792e52a-3ef9-4161-b868-c64f25adecfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------+-----+----------+-----------+\n|OrderID|CustomerID|Product|Quantity|Price| OrderDate|TotalAmount|\n+-------+----------+-------+--------+-----+----------+-----------+\n|   1005|       104|Monitor|       1|12000|2024-04-25|      12000|\n|   1004|       101|  Mouse|       3| 1000|2024-04-01|       3000|\n|   1003|       103|   Desk|       1|10000|2024-03-15|      10000|\n|   1002|       102| Mobile|       2|25000|2024-02-10|      50000|\n|   1001|       101| Laptop|       1|70000|2024-01-05|      70000|\n+-------+----------+-------+--------+-----+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. Sort orders by OrderDate in descending order\n",
    "df_ord.sort(df_ord.OrderDate, ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368a9a08-cb7c-4a9f-b941-77d6f79da41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9. Write the final result as a Parquet file partitioned by City\n",
    "df_joined.write.mode(\"overwrite\").parquet(r\"file:/Workspace/Shared/jun-13/df_joined\", partitionBy=\"City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b61d11-8993-488f-8a13-314cd91c49dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10. Create a temporary view and run Spark SQL:\n",
    "df_joined.createOrReplaceTempView(\"df_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b466bd-d030-4b56-a0ce-272a45dbeda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| Name|Sales|\n+-----+-----+\n|Aditi|73000|\n|Rohan|50000|\n|Meena|10000|\n|Kabir|12000|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 10.1 Total sales by customer\n",
    "spark.sql(\"\"\"\n",
    "          SELECT Name, SUM(TotalAmount) AS Sales FROM df_joined\n",
    "          GROUP BY CustomerID, Name\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f45e8c-5044-4044-a22f-6630b2b45411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|     City|ProductCount|\n+---------+------------+\n|Bangalore|           1|\n|   Mumbai|           2|\n|    Delhi|           1|\n|Hyderabad|           1|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10.2 Count of products per city\n",
    "spark.sql(\"\"\"\n",
    "          SELECT City, COUNT(DISTINCT(Product)) AS ProductCount FROM df_joined\n",
    "          GROUP BY City\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69ad766-6e3b-493b-8d55-936d5cd50c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n|  City|Revenue|\n+------+-------+\n|Mumbai|  73000|\n| Delhi|  50000|\n+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10.3 Top 2 cities by revenue\n",
    "spark.sql(\"\"\"\n",
    "          SELECT City, SUM(TotalAmount) AS Revenue FROM df_joined\n",
    "          GROUP BY City\n",
    "          ORDER BY SUM(TotalAmount) DESC\n",
    "          LIMIT 2\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b473b4-8335-4df1-b56c-3e420311a7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}