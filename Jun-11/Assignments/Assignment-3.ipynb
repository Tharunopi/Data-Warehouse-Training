{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd8754a-b206-4339-b336-58f8e0e7a3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7cbdad0-8066-4c4c-a01f-b8453b290b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1520322650531891#setting/sparkui/0611-042249-grg1r6w4/driver-1255471792126635555\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72d62252dd60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"NB-1\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "895f4e6e-179a-493c-8b88-232493b6d9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313e92a1-826f-458c-b78b-e4cd20f4f093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "(\"Ananya\", \"HR\", 52000),\n",
    "(\"Rahul\", \"Engineering\", 65000),\n",
    "(\"Priya\", \"Engineering\", 60000),\n",
    "(\"Zoya\", \"Marketing\", 48000),\n",
    "(\"Karan\", \"HR\", 53000),\n",
    "(\"Naveen\", \"Engineering\", 70000),\n",
    "(\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5aa553d-43a1-4e4e-88f1-01527a324952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "performance = [\n",
    "(\"Ananya\", 2023, 4.5),\n",
    "(\"Rahul\", 2023, 4.9),\n",
    "(\"Priya\", 2023, 4.3),\n",
    "(\"Zoya\", 2023, 3.8),\n",
    "(\"Karan\", 2023, 4.1),\n",
    "(\"Naveen\", 2023, 4.7),\n",
    "(\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766f7bf7-68de-4c35-bb51-2b6483b6b74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_data = [\n",
    "(\"Ananya\", \"HR Portal\", 120),\n",
    "(\"Rahul\", \"Data Platform\", 200),\n",
    "(\"Priya\", \"Data Platform\", 180),\n",
    "(\"Zoya\", \"Campaign Tracker\", 100),\n",
    "(\"Karan\", \"HR Portal\", 130),\n",
    "(\"Naveen\", \"ML Pipeline\", 220),\n",
    "(\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4061ad-ea37-42f2-ab07-0fded1578d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Joins and Advanced Aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854f5be4-21ba-4051-90ab-71a4c34ebc48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Join employee_data , performance_data , and project_data .\n",
    "df_joined = df.join(df_perf, on=\"Name\", how=\"inner\").join(df_proj, on=\"Name\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd51c0b-8164-4237-9cd4-495b9d05ee35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n| Department|Total Hours|\n+-----------+-----------+\n|         HR|        250|\n|Engineering|        600|\n|  Marketing|        190|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute total hours worked per department.\n",
    "df_joined.groupby(\"Department\").agg(\n",
    "    F.sum(\"HoursWorked\").alias(\"Total Hours\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef70b28-c9f2-4f29-991c-3219e967422b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n|         Project|Average rating|\n+----------------+--------------+\n|       HR Portal|           4.3|\n|   Data Platform|           4.6|\n|Campaign Tracker|           3.8|\n|     ML Pipeline|           4.7|\n+----------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute average rating per project.\n",
    "df_joined.groupby(\"Project\").agg(\n",
    "    F.round(F.mean(\"Rating\"), 1).alias(\"Average rating\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc1c566-912d-4e38-825c-fbc3c488dfcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Handling Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca7fc52-4c40-47c5-a01f-d1aa1b911ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n|Tharun|2023|  NULL|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Add a row to performance_data with a None rating.\n",
    "row = spark.createDataFrame(Row([\"Tharun\", 2023, None]), df_perf.schema)\n",
    "df_perf = df_perf.union(row)\n",
    "df_perf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd800413-b2af-4f17-98cb-d3491cb70df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Tharun|2023|  NULL|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Filter rows with null values.\n",
    "df_perf.filter(df_perf.Rating.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c55006-5b5c-4596-8322-aac8213a1fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n|Tharun|2023|   4.6|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Replace null ratings with the department average.\n",
    "deptAverage = df_joined.groupby(\"Department\").agg(\n",
    "    F.mean(\"Rating\").alias(\"Average Rating\")\n",
    ")\n",
    "deptAverage = deptAverage.filter(deptAverage.Department == \"Engineering\").first()[1]\n",
    "\n",
    "df_perf.fillna(round(deptAverage, 1), subset=\"Rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33695751-e1af-4a8a-a30a-33180e52be41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Built-In Functions and UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119aa288-dbe1-4d86-b0ec-9ed329727d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n|  Name|Rating|PerformanceCategory|\n+------+------+-------------------+\n|Ananya|   4.5|               Good|\n| Rahul|   4.9|          Excellent|\n| Priya|   4.3|               Good|\n|  Zoya|   3.8|            Average|\n| Karan|   4.1|               Good|\n|Naveen|   4.7|          Excellent|\n|Fatima|   3.9|            Average|\n+------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Create a column PerformanceCategory :\n",
    "# Excellent (>=4.7),\n",
    "# Good (4.0–4.69),\n",
    "# Average (<4.0)\n",
    "df_joined.withColumn(\n",
    "  \"PerformanceCategory\",\n",
    "  F.when(df_joined.Rating >= 4.7, \"Excellent\").when(df_joined.Rating < 4.0, \"Average\").otherwise(\"Good\")\n",
    "  ).select([\"Name\", \"Rating\", \"PerformanceCategory\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3589c8-03a9-46ea-b2e4-ea94a8564bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|Bonous|\n+------+-----------+------+----+------+----------------+-----------+------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|  5000|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200| 10000|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|  5000|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|  5000|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|  5000|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220| 10000|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|  5000|\n+------+-----------+------+----+------+----------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. Create a UDF to assign bonus:\n",
    "# If project hours > 200 → 10,000\n",
    "# Else → 5,000\n",
    "def bonousAssigner(project_hours):\n",
    "    if project_hours >= 200:\n",
    "        return 10_000\n",
    "    return 5_000\n",
    "\n",
    "bonous = F.udf(bonousAssigner)\n",
    "\n",
    "df_joined.withColumn(\"Bonous\", bonous(\"HoursWorked\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf802e4-0495-48c0-9565-b610b820941d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Date and Time Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d694e3-58e3-47c3-ad00-e54866346783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+------------+\n|  Name| Department|Salary|  JoinDate|MonthsWorked|\n+------+-----------+------+----------+------------+\n|Ananya|         HR| 52000|2021-06-01|        48.0|\n| Rahul|Engineering| 65000|2021-06-01|        48.0|\n| Priya|Engineering| 60000|2021-06-01|        48.0|\n|  Zoya|  Marketing| 48000|2021-06-01|        48.0|\n| Karan|         HR| 53000|2021-06-01|        48.0|\n|Naveen|Engineering| 70000|2021-06-01|        48.0|\n|Fatima|  Marketing| 45000|2021-06-01|        48.0|\n+------+-----------+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Add a column JoinDate with 2021-06-01 for all, then add MonthsWorked as difference from today.\n",
    "df = df.withColumn(\"JoinDate\", F.lit(\"2021-06-01\").cast(\"date\"))\n",
    "df = df.withColumn(\"MonthsWorked\", F.round(F.months_between(F.current_date(), df.JoinDate)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b6f889-bddf-4d0e-8aa2-4184cb94ebe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+------------+\n|  Name| Department|Salary|  JoinDate|MonthsWorked|\n+------+-----------+------+----------+------------+\n|Ananya|         HR| 52000|2021-06-01|        48.0|\n| Rahul|Engineering| 65000|2021-06-01|        48.0|\n| Priya|Engineering| 60000|2021-06-01|        48.0|\n|  Zoya|  Marketing| 48000|2021-06-01|        48.0|\n| Karan|         HR| 53000|2021-06-01|        48.0|\n|Naveen|Engineering| 70000|2021-06-01|        48.0|\n|Fatima|  Marketing| 45000|2021-06-01|        48.0|\n+------+-----------+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Calculate how many employees joined before 2022.\n",
    "df.filter(F.year(df.JoinDate) < 2022).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a18bf6cc-41fd-482c-8e1e-4dfc3d03e43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Unions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041ded52-3168-415e-a455-568aa764df26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+------------+\n|  Name| Department|Salary|  JoinDate|MonthsWorked|\n+------+-----------+------+----------+------------+\n|Ananya|         HR| 52000|2021-06-01|        48.0|\n| Rahul|Engineering| 65000|2021-06-01|        48.0|\n| Priya|Engineering| 60000|2021-06-01|        48.0|\n|  Zoya|  Marketing| 48000|2021-06-01|        48.0|\n| Karan|         HR| 53000|2021-06-01|        48.0|\n|Naveen|Engineering| 70000|2021-06-01|        48.0|\n|Fatima|  Marketing| 45000|2021-06-01|        48.0|\n| Meena|         HR| 48000|      NULL|        NULL|\n|   Raj|  Marketing| 51000|      NULL|        NULL|\n+------+-----------+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 11. Create another small team DataFrame and union() it with employee_data .\n",
    "extra_employees = [\n",
    "(\"Meena\", \"HR\", 48000, None, None),\n",
    "(\"Raj\", \"Marketing\", 51000, None, None)\n",
    "]\n",
    "small_team = spark.createDataFrame(extra_employees, df.schema)\n",
    "\n",
    "df = df.union(small_team)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f665464-29d0-42bf-9eb3-d2d7447aa02f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#**Saving Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cdaf54-418b-452b-bd19-ec62ab42a4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 12. Save the final merged dataset (all 3 joins) as a partitioned Parquet file based on Department .\n",
    "df_joined.write.mode(\"overwrite\").parquet(\"df_joined_Data\", partitionBy=\"Department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b79c24b0-9187-44a7-bf99-32e04df18f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}